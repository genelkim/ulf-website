<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Introduction to ULF Type Structure</title>
    <link href="/assets/css/styles.css" rel="stylesheet">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Introduction to ULF Type Structure</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Introduction to ULF Type Structure" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/2019/01/08/introduction-to-ulf-type-structure/" />
<meta property="og:url" content="http://localhost:4000/2019/01/08/introduction-to-ulf-type-structure/" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-08T00:00:00-05:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/2019/01/08/introduction-to-ulf-type-structure/","headline":"Introduction to ULF Type Structure","dateModified":"2019-01-08T00:00:00-05:00","datePublished":"2019-01-08T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/01/08/introduction-to-ulf-type-structure/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
    <a href="/contrib-authors.html" >Authors</a>
  
</nav>

    <h1>Introduction to ULF Type Structure</h1>
<p>08 Jan 2019 - 


    
        
        
        <a href="/authors/gene/">Gene Louis Kim</a>
        
    
        
        
        and <a href="/authors/len/">Lenhart Schubert</a>
        
    

</p>

<!-- %`````````````````````````````` -->

<!-- % \section{Episodic Logic and Unscoped Logical Form} -->
<!-- % %`````````````````````````````````````````````````` -->
<!-- % Episodic Logic (EL) has been under development for many years, -->
<!-- % and its syntactic and semantic features, the reasoning it enables  -->
<!-- % (using the {\sc Epilog} inference engine), and large-scale extraction -->
<!-- % of EL-encoded general factoids from text have been documented in  -->
<!-- % various publications (e.g., Hwang \& Schubert 1993, 2000; Schubert \& -->
<!-- % Hwang 2000; Schubert 2000, 2013; Schubert \& Tong 2013; Van Durme  -->
<!-- % \& Schubert 2008; Morbini \& Schubert 2009, 2013). The greatest -->
<!-- % remaining challenges in broad usage of EL and {\sc Epilog} in -->
<!-- % language understanding, dialogue and commonsense reasoning are -->
<!-- % the creation of a more accurate semantic parser and the acquisition -->
<!-- % of large amounts of lexical and world knowledge. While our existing -->
<!-- % work on knowledge acquisition is state-of-the-art (e.g., Van Durme -->
<!-- % et al.\ 2009, Kim \& Schubert 2016), its accuracy is limited by -->
<!-- % the quality of semantic parsing; this limitation provides part of  -->
<!-- % the impetus for the present proposal. -->

<p>The following six examples provide an idea of the language-like 
syntax of ULFs. The first two are from the Tatoeba database, the next
three are from <em>The Little Prince</em> (which was used for the first 
AMR-annotated corpus), and the last is from the Web:</p>

<ol>
  <li><span class="ex-sent">Could you dial for me?</span>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(((pres could.aux-v) you.pro ((dial.v {ref1}.pro) 
                           (adv-a (for.p me.pro)))) ?)
</code></pre></div>    </div>
  </li>
  <li><span class="ex-sent">If I were you I would be able to succeed.</span>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((if.ps (I.pro ((cf were.v) (= you.pro))))
 (I.pro ((cf will.aux-s) 
      (be.v (able.a (to succeed.v)))))) 
</code></pre></div>    </div>
  </li>
  <li><span class="ex-sent">He neglected three little bushes</span>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(he.pro ((past neglect.v) 
      (three.d (little.a (plur bush.n)))))
</code></pre></div>    </div>
  </li>
  <li><span class="ex-sent">Flowers are weak creatures</span>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((k (plur flower.n)) ((pres be.v) 
                   (weak.a (plur creature.n))))
</code></pre></div>    </div>
  </li>
  <li><span class="ex-sent">My drawing is not a picture of a hat</span>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((my.d drawing.n) ((pres be.v) not.adv-s
                (a.d (picture-of.n (a.d hat.n)))))
</code></pre></div>    </div>
  </li>
  <li><span class="ex-sent">Very few people still debate the fact that the earth is heating up</span>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(((fquan (very.adv-a few.a)) (plur person.n))
 (still.adv-s (debate.v
    (the.d (n+preds fact.n 
                    (= (that ((the.d |Earth|.n) 
((pres prog) heat_up.v)))))))))
</code></pre></div>    </div>
  </li>
</ol>

<p>As can be seen, ULF structure quite closely reflects phrase structure; 
and the type tags of atomic constituents, such as <tt>.pro, .v, .p, .a, 
.d, .n,</tt> etc., are intended to echo the part-of-speech origins of these 
constituents, such as <i>pronoun, verb, preposition, adjective, determiner,
  noun,</i> etc., respectively. Originally, ULFs contained some (\lambda)-abstracts,
for example to form a conjunctive predicate from postmodified nouns,
but we have introduced syntactic sugar elements that relieve annotators
from coding such abstracts. An example is seen in (6): The <tt>n+preds</tt>
macro takes a noun and one or more predicates as complements, and these
are expanded into a (\lambda)-abstracted conjunctive predicate in
postprocessing. As a result, ULFs are relatively amenable to human
creation and intuitive interpretation. Moreover, as mentioned in the 
Introduction, the proximity to surface structure enables NLog-like
inference and more.</p>

<p>
But then isn't parsing into ULF just another variant of syntactic
parsing? The essential difference is that the type tags correspond
to broad semantic categories (certain types of model-theoretic 
functions), and as such enable us to ensure that the type structure 
of ULFs -- their operator-operand combinations -- are semantically 
coherent. Richard Montague's profoundly influential work can be 
viewed as demonstrating the crucial importance of paying attention
to the semantic types of words and phrases, and that doing so leads 
to a view of language as very close to logic; as a result it lends
itself to inference, at least to the extent that we can resolve --
or are prepared to tolerate -- various forms of ambiguity, 
context-dependence and indexicality.

<p>
Our semantic types are not as high-order as Montague's, nor as "rigid"
as Montague's, but they suffice for maintaining type coherence. In
particular, quantification is first-order, i.e., it iterates over 
individual entities, not over predicates, etc.\ -- though through
reification of predicate meanings and sentence meanings, we can "talk
about" kinds of things, kinds of actions, propositions, etc., not just
ordinary objects.

<p>
As soon as we take semantic types seriously in ULFs like the above,
we see that certain type-shifting operators are needed to maintain type
coherence. For example, in sentence (1) the phrase <i>for me</i> is coded
as <tt>(adv-a (for.p me.pro))</tt>, rather than simply <tt>(for.p me.pro)</tt>.
That is because it is functioning here as a <i>predicate modifier</i>,
semantically operating on the verbal predicate <tt>(dial.v {ref1}.pro)</tt>
(<i>dial a certain thing</i>). Without the <tt>adv-a</tt> operator the 
prepositional phrase is just a 1-place predicate. Its use as a predicate 
is apparent in contexts like <i>"This puppy is for me"</i>. Note that
semantically the 1-place predicate <tt>(for.p me.pro)</tt> is formed by 
applying the 2-place predicate <tt>for.p</tt> to the (individual-denoting) 
term <tt>me.pro</tt>. (Viewing $n$-place predicates as successively applied 
to their arguments, each time reducing the adicity, is in keeping with 
the traditions of Sch\"{o}nfinkel, Church, Curry, Montague, and others -- 
hence "curried" predicates.) If we apply <tt>(for.p me.pro)</tt> to another 
argument, such as <tt>|Snoopy|</tt> (the name of a puppy), we obtain a truth 
value. So semantically, <tt>adv-a</tt> is a <i>type-shifting operator</i>
of type (<i>predicate</i> $\rightarrow$ (<i>predicate</i> $\rightarrow$
<i>predicate</i>))), where the predicates are 1-place and thus of type 
(<i>entity</i> $\rightarrow$ <i>truth value</i>). Of course, the name 
<tt>adv-a</tt> is intended to suggest "adverbial", in recognition of the 
grammatical distinction between predicative and adverbial uses of 
prepositional phrases.

<p>
In the preceding discussion we glossed over <i>intensionality</i>. For example,
(2) is a counterfactual conditional, and the consequent clause <i>"I
  would be able to succeed"</i> is not evaluated in the actual world, but
in a possible world where the (patently false) antecedent is imagined
to be true. ULF and deeper LFs derived from it are based on a semantics
where sentences are evaluated in <i>possible situations (episodes)</i>,
whose maxima are possible worlds. Details about syntactic forms and 
semantic types in our approach to LF have been provided in many past 
publications~\cite{hwang1992thesis,hwang1994ICTL,schubert2000book}.

<p>
There is scarcely space to say more about types in ULF here, but we note some 
further type-shifting operators in the examples: '<tt>to</tt>' (synonym: <tt>ka</tt>)
in (2) shifts a verbal predicate to a <i>kind (type) of action or attribute</i>,
which is an abstract individual; '<tt>k</tt>' in (4) shifts a nominal predicate
to a <i>kind</i> of thing (so the subject here is the abstract kind,
flowers, whose instances consist of sets of flowers; and `<tt>that</tt>' in (6)
produces a reified <i>proposition</i> (again an abstract individual) from
a sentence meaning. Through these type shifts, we are able to maintain a
simple, classical view of predication, while allowing greater expressivity
than the most widely employed logical forms, for example enabling generalized 
quantification (as in (6)), modification, reification, and other forms of 
intensionality.

<p>
The positioning of <tt>(adv-a (for.p me.pro))</tt> within the verbal predicate
it modifies, rather than in prefix-operator position, already indicates
a certain looseness in the ULF syntax, as opposed to the rigidity of formal
logic. This is unproblematic because we restrict the way operators may
combine with operands so that type consistency is assured  -- and in fact 
in subsequent processing, any <tt>(adv-a (...))</tt> constituents of a verbal 
predicate are moved so as to immediately precede that predicate. There
are a number of further kinds of looseness in ULFs, but we defer further
discussion to the next subsection.

<p>
Finally we note a general concern that might be raised about ULFs.
Since they largely conform with surface syntax, they are clearly
language-specific. Isn't the point of semantics to get at the
deeper meanings underlying the surface forms or language, and shouldn't
these be somewhat uniform across languages? Our answer is two-fold:
First, from a semantic perspective, the ULFs for different languages
will have certain essential commonalities, namely, means to express
predication, truth-functional and other connectives, generalized
quantifiers, predicate and sentence modification, predicate and 
sentence reification, implicit and explicit reference to events/
situations, comparatives, and a few other devices. Surface order is
less important than these semantic commonalities. Second, we do
think that sentence meanings should be factored into (as far as 
possible) minimal, separately usable, canonical propositions. This 
seems plausible both from speculations in cognitive science about 
"Mentalese", and from a practical perspective, since canonicalization
ensures that connections between ideas can be quickly recognized and 
used for inference. A glimpse of the canonicalization process was
already seen above for the sentence <i>"The boy wants to go"</i>.
Of course the meaning of sentences "in the wild" can be much more 
complex and subtle. Our hypothesis is that we can conquer those 
complexities effectively by starting with a type-coherent surface form, 
and systematically deriving canonical forms, bringing to bear many 
different kinds of influential factors. The next subsection elaborates 
on this view.
</p></p></p></p></p></p></p>


  </body>
</html>
